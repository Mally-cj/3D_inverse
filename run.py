"""
æ•°æ®æ¥æºè¯´æ˜ï¼š
- S_obs: çœŸå®é‡å¤–åœ°éœ‡è§‚æµ‹æ•°æ® (PSTM_resample1_lf_extension2.sgy)
- Z_full: æµ‹äº•æ’å€¼é˜»æŠ—æ•°æ® (yyf_smo_train_Volume_PP_IMP.sgy)
  * äº•ä½å¤„ï¼šçœŸå®æµ‹äº•æ•°æ®ï¼ˆ8å£äº•çš„ç²¾ç¡®é˜»æŠ—å€¼ï¼‰
  * å…¶ä»–ä½ç½®ï¼šæ’å€¼ä¼°è®¡å€¼ï¼ˆå¯ç”¨ä½†ä¸å¤Ÿç²¾ç¡®ï¼‰
- M_mask: äº•ä½æ©ç ï¼Œæ ‡è®°æ•°æ®å¯ä¿¡åº¦åˆ†å¸ƒ
- ä¸¤é˜¶æ®µç®—æ³•å……åˆ†åˆ©ç”¨çœŸå®è§‚æµ‹æ•°æ®å’Œäº•ä½çº¦æŸ
"""

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import sys
import torch.optim
from Model.net2D import UNet, forward_model
from Model.utils import *
from torch.utils import data
from Model.joint_well import *
import matplotlib.pyplot as plt
import numpy as np
import pylops
from pylops.utils.wavelets import ricker
from scipy.signal import filtfilt
from scipy import signal
import torch.nn.functional as F
from scipy.signal.windows import gaussian
from obspy.io.segy.segy import _read_segy
import pdb
import sys
sys.path.append('..')
# import data_tools as tools
from icecream import ic 
sys.path.append('../codes')
sys.path.append('deep_learning_impedance_inversion_chl')
from cpp_to_py import generate_well_mask as generate_well_mask2
from cpp_to_py import get_wellline_and_mask as get_wellline_and_mask2
import psutil
import gc
from tqdm import tqdm

# è®­ç»ƒ/æµ‹è¯•æ¨¡å¼åˆ‡æ¢
Train = False  # è®¾ç½®ä¸º True è¿›è¡Œè®­ç»ƒå¹¶ä¿å­˜Forwardç½‘ç»œæƒé‡ï¼›è®¾ç½®ä¸º False è¿›è¡Œæ¨ç†æµ‹è¯•

# ğŸ“ é‡è¦è¯´æ˜ï¼š
# - é¦–æ¬¡ä½¿ç”¨æ—¶ï¼Œå»ºè®®å…ˆè®¾ç½® Train = True è¿è¡Œä¸€æ¬¡è®­ç»ƒï¼Œç”ŸæˆForwardç½‘ç»œæƒé‡æ–‡ä»¶
# - Forwardç½‘ç»œå­¦ä¹ æ•°æ®é©±åŠ¨çš„æœ€ä¼˜å­æ³¢ï¼Œå¯¹åæ¼”ç²¾åº¦è‡³å…³é‡è¦
# - è®­ç»ƒå®Œæˆåï¼Œå¯è®¾ç½® Train = False è¿›è¡Œæ¨ç†æµ‹è¯•

# æ™ºèƒ½è®¾å¤‡æ£€æµ‹å’Œå‚æ•°é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ Using device: {device}")

# æ ¹æ®è®¾å¤‡è‡ªåŠ¨è°ƒæ•´å‚æ•°
if device.type == 'cuda':
    print("ğŸ“Š GPU mode: Using full dataset")
    dtype = torch.cuda.FloatTensor
    # GPUå‚æ•°é…ç½®
    USE_FULL_DATA = True
    MAX_SPATIAL_SLICES = 251  # å®Œæ•´æ•°æ®
    BATCH_SIZE = 10
    PATCH_SIZE = 70
    N_WELL_PROFILES = 30
    ADMM_ITER = 100
    ADMM_ITER1 = 50
    MAX_TRAIN_SAMPLES = None  # ä¸é™åˆ¶
else:
    print("ğŸ’» CPU mode: Using optimized subset")
    dtype = torch.FloatTensor
    # CPUä¼˜åŒ–å‚æ•°é…ç½®
    USE_FULL_DATA = False
    MAX_SPATIAL_SLICES = 50   # å‡å°‘æ•°æ®é‡
    BATCH_SIZE = 1
    PATCH_SIZE = 48
    N_WELL_PROFILES = 10
    ADMM_ITER = 30
    ADMM_ITER1 = 15
    MAX_TRAIN_SAMPLES = 300

print(f"ğŸ“‹ Configuration:")
print(f"  - Spatial slices: {MAX_SPATIAL_SLICES}")
print(f"  - Batch size: {BATCH_SIZE}")
print(f"  - Patch size: {PATCH_SIZE}")
print(f"  - Training samples: {MAX_TRAIN_SAMPLES if MAX_TRAIN_SAMPLES else 'unlimited'}")
print(f"  - Training iterations: {ADMM_ITER} + {ADMM_ITER1}")


#############################################################################################################
### ç¬¬2éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½ - åŒºåˆ†æ¼”ç¤ºæ•°æ®å’Œå·¥ç¨‹æ•°æ®
#############################################################################################################

print("\n" + "="*80) 
print("ğŸ“‚ ç¬¬2éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½")
print("="*80)

print("\nğŸ”„ åŠ è½½å·¥ç¨‹ç”¨å®Œæ•´é˜»æŠ—æ•°æ® (è®­ç»ƒç”¨)...")
print("   è¯´æ˜ï¼šæ˜¯ç”¨æµ‹äº•æ•°æ®æ’å€¼åçš„å®Œæ•´é˜»æŠ—ï¼Œäº•ä½å¤„ç²¾ç¡®ï¼Œå…¶ä»–ä½ç½®æ’å€¼ä¼°è®¡")
# å·¥ç¨‹æ•°æ®ï¼šæµ‹äº•æ’å€¼å¾—åˆ°çš„å®Œæ•´é˜»æŠ—ï¼ˆè¿™æ˜¯å®é™…å·¥ç¨‹çš„èµ·ç‚¹ï¼‰
segy = _read_segy("data/yyf_smo_train_Volume_PP_IMP.sgy")
impedance_model_full = []
for i in range(0, len(segy.traces)):
    impedance_model_full.append(segy.traces[i].data)

impedance_model_full = np.array(impedance_model_full).reshape(251, len(impedance_model_full)//251, 601).transpose(2, 1, 0)

# æ ¹æ®è®¾å¤‡é…ç½®è°ƒæ•´æ•°æ®å¤§å°
if not USE_FULL_DATA:
    impedance_model_full = impedance_model_full[:, :MAX_SPATIAL_SLICES, :]

impedance_model_full = np.log(impedance_model_full)
print(f"âœ… å®Œæ•´é˜»æŠ—æ•°æ®åŠ è½½å®Œæˆ: {impedance_model_full.shape}")
print(f"   æ•°æ®æ„æˆï¼šäº•ä½å¤„ä¸ºçœŸå®æµ‹äº•å€¼ï¼Œå…¶ä»–ä½ç½®ä¸ºæ’å€¼ä¼°è®¡å€¼")

print("\nğŸŒŠ ä»å®Œæ•´é˜»æŠ—æ•°æ®æå–ä½é¢‘èƒŒæ™¯...")
# ä½é¢‘èƒŒæ™¯é˜»æŠ—ï¼šä»å®Œæ•´é˜»æŠ—æ•°æ®ä¸­æå–ä½é¢‘æˆåˆ†
Z_back = []
for i in range(impedance_model_full.shape[2]):
    B, A = signal.butter(2, 0.012, 'low')  # æˆªæ­¢é¢‘ç‡çº¦12Hz
    m_loww = signal.filtfilt(B, A, impedance_model_full[..., i].T).T
    nsmooth = 3
    m_low = filtfilt(np.ones(nsmooth)/float(nsmooth), 1, m_loww)  # æ—¶é—´æ–¹å‘å¹³æ»‘
    nsmooth = 3
    m_low = filtfilt(np.ones(nsmooth)/float(nsmooth), 1, m_low.T).T  # ç©ºé—´æ–¹å‘å¹³æ»‘
    Z_back.append(m_low[..., None])
Z_back = np.concatenate(Z_back, axis=2)
print(f"âœ… ä½é¢‘èƒŒæ™¯é˜»æŠ—ç”Ÿæˆå®Œæˆ: {Z_back.shape}")
print(f"   ç”¨é€”ï¼šä¸ºæœ€å°äºŒä¹˜åˆå§‹åŒ–æä¾›ä½é¢‘çº¦æŸ")

print("\nğŸŒŠ åŠ è½½çœŸå®è§‚æµ‹åœ°éœ‡æ•°æ®...")
# è§‚æµ‹åœ°éœ‡æ•°æ®ï¼šç›´æ¥åŠ è½½é‡å¤–è§‚æµ‹çš„åœ°éœ‡æ•°æ®
print("   ğŸ“‚ åŠ è½½PSTMåœ°éœ‡æ•°æ®æ–‡ä»¶...")
segy_seismic = _read_segy("data/PSTM_resample1_lf_extension2.sgy")
S_obs = []
for i in range(0, len(segy_seismic.traces)):
    S_obs.append(segy_seismic.traces[i].data)

S_obs = np.array(S_obs).reshape(251, len(S_obs)//251, 601).transpose(2, 1, 0)

# æ ¹æ®è®¾å¤‡é…ç½®è°ƒæ•´æ•°æ®å¤§å°
if not USE_FULL_DATA:
    S_obs = S_obs[:, :MAX_SPATIAL_SLICES, :]

print(f"âœ… çœŸå®è§‚æµ‹åœ°éœ‡æ•°æ®åŠ è½½å®Œæˆ: {S_obs.shape}")
print(f"   æ•°æ®æ¥æºï¼šé‡å¤–åœ°éœ‡å‹˜æ¢è§‚æµ‹æ•°æ®")
print(f"   æ•°æ®ç”¨é€”ï¼šä½œä¸ºåæ¼”ç®—æ³•çš„è§‚æµ‹çº¦æŸæ¡ä»¶")

#############################################################################################################
### ç¬¬3éƒ¨åˆ†ï¼šäº•ä½æ•°æ®å¤„ç† - å®šä¹‰äº•ä½å’Œç”Ÿæˆæ©ç 
#############################################################################################################

print("\n" + "="*80)
print("ğŸ¯ ç¬¬3éƒ¨åˆ†ï¼šäº•ä½æ•°æ®å¤„ç†") 
print("="*80)

# ç½‘æ ¼å‚æ•°
nx, ny = S_obs.shape[1:3]
basex = 450
basey = 212

# å·²çŸ¥æµ‹äº•ä½ç½®çš„ç»å¯¹åæ ‡ï¼ˆè¿™äº›ä½ç½®æœ‰å‡†ç¡®çš„æµ‹äº•æ•°æ®ï¼‰
print("ğŸ“ å®šä¹‰å·²çŸ¥æµ‹äº•ä½ç½®...")

if not USE_FULL_DATA:
    # CPUæ¨¡å¼ï¼šä½¿ç”¨é€‚åˆç¼©å‡ç½‘æ ¼çš„äº•ä½
    print("   ğŸ’» CPUæ¨¡å¼ï¼šä½¿ç”¨é€‚é…çš„äº•ä½é…ç½®")
    well_pos = [[10, 10], [20, 20], [30, 30], [40, 40]]  # é€‚åˆ(50, 251)ç½‘æ ¼çš„äº•ä½
else:
    # GPUæ¨¡å¼ï¼šä½¿ç”¨åŸå§‹å®Œæ•´äº•ä½
    print("   ğŸ–¥ï¸  GPUæ¨¡å¼ï¼šä½¿ç”¨å®Œæ•´äº•ä½é…ç½®")
    pos = [[594,295], [572,692], [591,996], [532,1053], [603,1212], [561,842], [504,846], [499,597]]
    # è½¬æ¢ä¸ºç›¸å¯¹ç½‘æ ¼åæ ‡
    well_pos = [[y-basey, x-basex] for [x, y] in pos]

print(f"âœ… äº•ä½ä¿¡æ¯:")
print(f"   - æµ‹äº•ä½ç½®æ•°é‡: {len(well_pos)}")
print(f"   - äº•ä½åæ ‡ (ç½‘æ ¼): {well_pos}")
print(f"   - è¿™äº›ä½ç½®çš„é˜»æŠ—æ•°æ®æ˜¯å‡†ç¡®çš„ï¼ˆçœŸå®æµ‹äº•å€¼ï¼‰")
print(f"   - å…¶ä»–ä½ç½®çš„é˜»æŠ—æ•°æ®é€šè¿‡æ’å€¼è·å¾—ï¼ˆä¼°è®¡å€¼ï¼‰")

print("\nğŸ¯ ç”Ÿæˆäº•ä½æ©ç ...")
# ç”Ÿæˆäº•ä½æ©ç ï¼šæ ‡è®°æ•°æ®å¯ä¿¡åº¦åˆ†å¸ƒ
grid_shape = S_obs.shape[1:3]
M_well_mask_dict = generate_well_mask2(well_pos, grid_shape, well_range=15, sigma=5)

# å°†å­—å…¸è½¬æ¢ä¸º2Dæ•°ç»„æ ¼å¼
M_well_mask = np.zeros(grid_shape)
for (line, cmp), weight in M_well_mask_dict.items():
    M_well_mask[line, cmp] = weight

print(f"âœ… äº•ä½æ©ç ç”Ÿæˆå®Œæˆ:")
print(f"   - ç½‘æ ¼å½¢çŠ¶: {grid_shape}")
print(f"   - äº•ä½å½±å“èŒƒå›´: 15ä¸ªç½‘æ ¼ç‚¹")
print(f"   - æ©ç å½¢çŠ¶: {M_well_mask.shape}")
print(f"   - äº•ä½æ•°é‡: {len(M_well_mask_dict)}")
print(f"   - æ©ç å€¼èŒƒå›´: [{M_well_mask.min():.3f}, {M_well_mask.max():.3f}]")
print(f"   - æ©ç å€¼å«ä¹‰:")
print(f"     * M=1.0: äº•ä½å¤„ï¼Œå®Œæ•´é˜»æŠ—æ•°æ®ä¸ºçœŸå®æµ‹äº•å€¼")
print(f"     * M=0.0: è¿œç¦»äº•ä½ï¼Œå®Œæ•´é˜»æŠ—æ•°æ®ä¸ºæ’å€¼ä¼°è®¡å€¼")
print(f"     * Mâˆˆ(0,1): äº•å½±å“èŒƒå›´ï¼Œå¯ä¿¡åº¦æ¸å˜è¿‡æ¸¡")

#############################################################################################################
### ç¬¬4éƒ¨åˆ†ï¼šè®­ç»ƒæ•°æ®æ„å»º - ä¸‰æ­¥éª¤ç”Ÿæˆè®­ç»ƒæ ·æœ¬
#############################################################################################################

print("\n" + "="*80)
print("ğŸ“¦ ç¬¬4éƒ¨åˆ†ï¼šè®­ç»ƒæ•°æ®æ„å»º")
print("="*80)

print("ğŸ”— æ­¥éª¤1ï¼šç”Ÿæˆéšæœºè¿äº•å‰–é¢...")
print(f"   - åŸºäº{len(well_pos)}å£äº•ç”Ÿæˆ{N_WELL_PROFILES}æ¡éšæœºè¿æ¥è·¯å¾„")
print(f"   - æ¯æ¡å‰–é¢å‚ç›´é«˜åº¦: 601ä¸ªæ—¶é—´é‡‡æ ·ç‚¹")
print(f"   - æ°´å¹³é•¿åº¦: å˜é•¿ï¼ˆæ ¹æ®äº•é—´è·¯å¾„å†³å®šï¼‰")

# è®­ç»ƒäº•ä½ï¼ˆæ·»åŠ æ ‡ç­¾ç”¨äºè·¯å¾„ç”Ÿæˆï¼‰
train_well = add_labels(well_pos)
extension_length = 10  # è·¯å¾„å»¶æ‹“é•¿åº¦

# å­˜å‚¨å„ç±»å‰–é¢æ•°æ®
Z_back_profiles = []       # ä½é¢‘èƒŒæ™¯å‰–é¢
Z_full_profiles = []       # å®Œæ•´é˜»æŠ—å‰–é¢  
S_obs_profiles = []        # è§‚æµ‹åœ°éœ‡æ•°æ®å‰–é¢
M_mask_profiles = []       # äº•ä½æ©ç å‰–é¢
path_coords = []           # å‰–é¢è·¯å¾„åæ ‡

print("   æ­£åœ¨ç”Ÿæˆè¿äº•å‰–é¢...")
for i in tqdm(range(N_WELL_PROFILES), desc="ç”Ÿæˆå‰–é¢"):
    # ç”Ÿæˆç¬¬iæ¡éšæœºè¿äº•å‰–é¢çš„åæ ‡ç‚¹
    interpolated_points, vMask = get_wellline_and_mask2(well_pos, grid_shape, M_well_mask_dict)
    
    # è®°å½•è·¯å¾„ä¿¡æ¯
    path_coords.append(interpolated_points)
    
    # æ‰©å±•æ©ç åˆ°æ—¶é—´ç»´åº¦ (601 Ã— å‰–é¢é•¿åº¦)
    vMask_time_extended = np.tile(vMask, (601, 1))
    M_mask_profiles.append(vMask_time_extended)
    
    # æ­¥éª¤2ï¼šæ²¿è¿äº•å‰–é¢æå–å„ç±»æ•°æ®
    # æå–ä½é¢‘èƒŒæ™¯æ²¿å‰–é¢çš„æ•°æ®
    Z_back_profiles.append(Z_back[:, interpolated_points[:, 0], interpolated_points[:, 1]])
    
    # æå–å®Œæ•´é˜»æŠ—æ²¿å‰–é¢çš„æ•°æ®ï¼ˆå…³é”®ï¼šè¿™æ˜¯è®­ç»ƒç›®æ ‡ï¼‰
    Z_full_profiles.append(impedance_model_full[:, interpolated_points[:, 0], interpolated_points[:, 1]])
    
    # æå–è§‚æµ‹åœ°éœ‡æ•°æ®æ²¿å‰–é¢çš„æ•°æ®
    S_obs_profiles.append(S_obs[:, interpolated_points[:, 0], interpolated_points[:, 1]])

print(f"âœ… æ­¥éª¤1&2å®Œæˆï¼šç”Ÿæˆ{N_WELL_PROFILES}æ¡ä¼ª2Då‰–é¢")
print(f"   æ¯æ¡å‰–é¢åŒ…å«4ç±»æ•°æ®ï¼š")
print(f"   - S_obs: è§‚æµ‹åœ°éœ‡æ•°æ® (ç”¨äºç‰©ç†çº¦æŸ)")
print(f"   - Z_full: å®Œæ•´é˜»æŠ—æ•°æ® (è®­ç»ƒç›®æ ‡ï¼Œç»“åˆæ©ç ä½¿ç”¨)")
print(f"   - Z_back: ä½é¢‘èƒŒæ™¯é˜»æŠ— (ç”¨äºæœ€å°äºŒä¹˜åˆå§‹åŒ–)")
print(f"   - M: äº•ä½æ©ç  (æ ‡è®°æ•°æ®å¯ä¿¡åº¦)")

print(f"\nğŸ“¦ æ­¥éª¤3ï¼šæ»‘çª—åˆ‡åˆ†ç»Ÿä¸€å°ºå¯¸...")
print(f"   - è¾“å…¥: {N_WELL_PROFILES}æ¡å˜é•¿å‰–é¢ (601Ã—å˜é•¿)")
print(f"   - è¾“å‡º: ç»Ÿä¸€å°ºå¯¸è®­ç»ƒå— (601Ã—{PATCH_SIZE})")
print(f"   - é‡å æ­¥é•¿: 5ä¸ªç‚¹ (æ•°æ®å¢å¼º)")

patchsize = PATCH_SIZE
oversize = 5

# å­˜å‚¨åˆ‡åˆ†åçš„è®­ç»ƒæ•°æ®
Z_back_patches = []       # ä½é¢‘èƒŒæ™¯è®­ç»ƒå—
Z_full_patches = []       # å®Œæ•´é˜»æŠ—è®­ç»ƒå—
S_obs_patches = []        # åœ°éœ‡æ•°æ®è®­ç»ƒå—
M_mask_patches = []       # äº•ä½æ©ç è®­ç»ƒå—

print("   æ­£åœ¨åˆ‡åˆ†è®­ç»ƒå—...")
for i in tqdm(range(N_WELL_PROFILES), desc="åˆ‡åˆ†æ•°æ®"):
    # ä½¿ç”¨æ»‘çª—æ–¹å¼å°†æ¯æ¡å‰–é¢åˆ‡åˆ†æˆå¤šä¸ªå›ºå®šå°ºå¯¸çš„è®­ç»ƒå—
    Z_back_patches.append(torch.tensor(image2cols(Z_back_profiles[i], (S_obs.shape[0], patchsize), (1, oversize))))
    Z_full_patches.append(torch.tensor(image2cols(Z_full_profiles[i], (S_obs.shape[0], patchsize), (1, oversize))))
    S_obs_patches.append(torch.tensor(image2cols(S_obs_profiles[i], (S_obs.shape[0], patchsize), (1, oversize))))
    M_mask_patches.append(torch.tensor(image2cols(M_mask_profiles[i], (S_obs.shape[0], patchsize), (1, oversize))))

# æ‹¼æ¥æ‰€æœ‰è®­ç»ƒå— [N_samples, 1, 601, PATCH_SIZE]
Z_back_train_set = torch.cat(Z_back_patches, 0)[..., None].permute(0, 3, 1, 2).type(dtype)
Z_full_train_set = torch.cat(Z_full_patches, 0)[..., None].permute(0, 3, 1, 2).type(dtype)
S_obs_train_set = torch.cat(S_obs_patches, 0)[..., None].permute(0, 3, 1, 2).type(dtype)
M_mask_train_set = torch.cat(M_mask_patches, 0)[..., None].permute(0, 3, 1, 2).type(dtype)

print(f"âœ… æ­¥éª¤3å®Œæˆï¼šç”Ÿæˆç»Ÿä¸€è®­ç»ƒæ•°æ®é›†")
print(f"   - è®­ç»ƒæ ·æœ¬æ€»æ•°: {len(S_obs_train_set)}")
print(f"   - æ¯ä¸ªæ ·æœ¬å¤§å°: {S_obs_train_set.shape[2]}Ã—{S_obs_train_set.shape[3]} (æ—¶é—´Ã—ç©ºé—´)")
print(f"   - æ•°æ®ç±»å‹: 4ç±» (åœ°éœ‡ã€é˜»æŠ—ã€èƒŒæ™¯ã€æ©ç )")

#############################################################################################################
### ç¬¬5éƒ¨åˆ†ï¼šæ•°æ®å½’ä¸€åŒ–å’Œæ•°æ®åŠ è½½å™¨
#############################################################################################################

print("\n" + "="*80)
print("ğŸ”§ ç¬¬5éƒ¨åˆ†ï¼šæ•°æ®å½’ä¸€åŒ–å’ŒåŠ è½½å™¨æ„å»º")
print("="*80)

# è®¡ç®—å½’ä¸€åŒ–å‚æ•°ï¼ˆåŸºäºå®Œæ•´é˜»æŠ—æ•°æ®çš„èŒƒå›´ï¼‰
logimpmax = impedance_model_full.max()
logimpmin = impedance_model_full.min()
print(f"ğŸ“Š é˜»æŠ—æ•°æ®èŒƒå›´: [{logimpmin:.3f}, {logimpmax:.3f}]")

# è®­ç»ƒæ•°æ®å½’ä¸€åŒ–
print("ğŸ”„ å½’ä¸€åŒ–è®­ç»ƒæ•°æ®...")
Z_full_norm = (Z_full_train_set - logimpmin) / (logimpmax - logimpmin)  # é˜»æŠ—å½’ä¸€åŒ–åˆ°[0,1]
S_obs_norm = 2 * (S_obs_train_set - S_obs_train_set.min()) / (S_obs_train_set.max() - S_obs_train_set.min()) - 1  # åœ°éœ‡æ•°æ®å½’ä¸€åŒ–åˆ°[-1,1]
Z_back_norm = (Z_back_train_set - logimpmin) / (logimpmax - logimpmin)  # ä½é¢‘èƒŒæ™¯å½’ä¸€åŒ–åˆ°[0,1]
# æ©ç ä¸éœ€è¦å½’ä¸€åŒ–ï¼Œä¿æŒ[0,1]èŒƒå›´

# åº”ç”¨è®­ç»ƒæ ·æœ¬é™åˆ¶ï¼ˆCPUæ¨¡å¼ï¼‰
if MAX_TRAIN_SAMPLES is not None:
    print(f"ğŸ”„ CPUæ¨¡å¼ï¼šé™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡åˆ° {MAX_TRAIN_SAMPLES}...")
    total_samples = len(S_obs_norm)
    if total_samples > MAX_TRAIN_SAMPLES:
        indices = torch.randperm(total_samples)[:MAX_TRAIN_SAMPLES]
        S_obs_norm = S_obs_norm[indices]
        Z_full_norm = Z_full_norm[indices]
        Z_back_norm = Z_back_norm[indices]
        M_mask_train_set = M_mask_train_set[indices]
        print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°é‡: {total_samples} â†’ {len(S_obs_norm)}")

print("ğŸ”§ å‡†å¤‡æµ‹è¯•æ•°æ®...")
# æµ‹è¯•æ•°æ®å½’ä¸€åŒ–ï¼ˆå…¨å°ºå¯¸æ•°æ®ï¼‰
S_obs_test_norm = 2 * (S_obs - S_obs.min()) / (S_obs.max() - S_obs.min()) - 1
Z_back_test_norm = (Z_back - logimpmin) / (logimpmax - logimpmin)
Z_full_test_norm = (impedance_model_full - logimpmin) / (logimpmax - logimpmin)
Z_true_test_norm = (impedance_model_full - logimpmin) / (logimpmax - logimpmin)  # ä»…ç”¨äºè¯„ä¼°

# è½¬æ¢ä¸ºPyTorchå¼ é‡æ ¼å¼
test_S_obs = torch.tensor(S_obs_test_norm[..., None]).permute(2, 3, 0, 1).type(dtype)
test_Z_back = torch.tensor(Z_back_test_norm[..., None]).permute(2, 3, 0, 1).type(dtype)
test_Z_full = torch.tensor(Z_full_test_norm[..., None]).permute(2, 3, 0, 1).type(dtype)
test_Z_true = torch.tensor(Z_true_test_norm[..., None]).permute(2, 3, 0, 1).type(dtype)

print("ğŸ“¦ æ„å»ºæ•°æ®åŠ è½½å™¨...")
# æ„å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ˆä¿®æ­£ï¼šç¡®ä¿æ•°æ®å¯¹åº”å…³ç³»æ­£ç¡®ï¼‰
Train_loader = data.DataLoader(
    data.TensorDataset(
        S_obs_norm,          # è§‚æµ‹åœ°éœ‡æ•°æ® - ç”¨äºUNetè¾“å…¥å’Œç‰©ç†çº¦æŸ
        Z_full_norm,         # å®Œæ•´é˜»æŠ—æ•°æ® - è®­ç»ƒç›®æ ‡ï¼ˆç»“åˆæ©ç ï¼‰
        Z_back_norm,         # ä½é¢‘èƒŒæ™¯é˜»æŠ— - ç”¨äºæœ€å°äºŒä¹˜åˆå§‹åŒ–
        M_mask_train_set     # äº•ä½æ©ç  - æ ‡è®°æ•°æ®å¯ä¿¡åº¦æƒé‡
    ),
    batch_size=BATCH_SIZE,
    shuffle=True
)

# æµ‹è¯•æ•°æ®åŠ è½½å™¨
Test_loader = data.DataLoader(
    data.TensorDataset(
        test_S_obs,          # æµ‹è¯•åœ°éœ‡æ•°æ®
        test_Z_full,         # æµ‹è¯•å®Œæ•´é˜»æŠ—
        test_Z_back,         # æµ‹è¯•ä½é¢‘èƒŒæ™¯
        test_Z_true          # æµ‹è¯•çœŸå®é˜»æŠ—ï¼ˆä»…ç”¨äºè¯„ä¼°ï¼‰
    ),
    batch_size=BATCH_SIZE,
    shuffle=False,
    drop_last=False
)

print(f"âœ… æ•°æ®åŠ è½½å™¨æ„å»ºå®Œæˆ:")
print(f"   - è®­ç»ƒæ‰¹å¤§å°: {BATCH_SIZE}")
# print(f"   - è®­ç»ƒæ‰¹æ•°: {l# è¿è¡Œä¿®æ­£ç‰ˆè®­ç»ƒ
# python seismic_imp_2D_high_channel_model_bgp_corrected.py

#############################################################################################################
### ç¬¬6éƒ¨åˆ†ï¼šå­æ³¢åˆå§‹åŒ–å’Œæ•°å­¦ç®—å­å®šä¹‰
#############################################################################################################

print("\n" + "="*80)
print("ğŸ”§ ç¬¬6éƒ¨åˆ†ï¼šå­æ³¢åˆå§‹åŒ–å’Œç®—å­å®šä¹‰")
print("="*80)

# åˆå§‹å­æ³¢ç”Ÿæˆï¼ˆç”¨äºåŠ é€Ÿå­æ³¢æ¨¡å—æ”¶æ•›ï¼‰
print("ğŸŒŠ ç”Ÿæˆåˆå§‹å­æ³¢...")
wav0 = wavelet_init(S_obs_norm.cpu().type(torch.float32), 101).squeeze().numpy()
size = S_obs.shape[0]

# æ„å»ºå·ç§¯çŸ©é˜µ
W = pylops.utils.signalprocessing.convmtx(wav0, size, len(wav0) // 2)[:size]
W = torch.tensor(W, dtype=torch.float32, device=device)

# é«˜æ–¯çª—å‡½æ•°ï¼ˆç”¨äºå­æ³¢å¹³æ»‘ï¼‰
N = len(wav0)
fp = 30  # ä¸»é¢‘
fs = 1000  # é‡‡æ ·é¢‘ç‡
std = int((fs/fp)/2)  # æ ‡å‡†å·®
gaussian_window = gaussian(N, std)
gaus_win = torch.tensor(gaussian_window[None, None, :, None]).type(dtype)

print(f"âœ… å­æ³¢åˆå§‹åŒ–å®Œæˆ:")
print(f"   - åˆå§‹å­æ³¢é•¿åº¦: {len(wav0)}")
print(f"   - å·ç§¯çŸ©é˜µå¤§å°: {W.shape}")
print(f"   - é«˜æ–¯çª—å‚æ•°: std={std}")

print("ğŸ”§ å®šä¹‰æ•°å­¦ç®—å­...")
# é˜»æŠ—å·®åˆ†ç®—å­ï¼ˆè®¡ç®—åå°„ç³»æ•°ï¼‰
def DIFFZ(z):
    """
    è®¡ç®—é˜»æŠ—çš„ç©ºé—´æ¢¯åº¦ï¼Œå¾—åˆ°åå°„ç³»æ•°
    è¾“å…¥: z - é˜»æŠ—æ•°æ® [batch, channel, time, space]
    è¾“å‡º: DZ - åå°„ç³»æ•° [batch, channel, time, space]
    """
    DZ = torch.zeros([z.shape[0], z.shape[1], z.shape[2], z.shape[3]], device=device).type(dtype)
    DZ[..., :-1, :] = 0.5 * (z[..., 1:, :] - z[..., :-1, :])
    return DZ



# æ€»å˜åˆ†æ­£åˆ™åŒ–æŸå¤±å‡½æ•°
def tv_loss(x, alfa):
    """
    æ€»å˜åˆ†æ­£åˆ™åŒ–æŸå¤±ï¼Œä¿æŒç©ºé—´è¿ç»­æ€§
    è¾“å…¥: x - é¢„æµ‹é˜»æŠ— [batch, channel, time, space]
          alfa - æ­£åˆ™åŒ–æƒé‡
    è¾“å‡º: TVæŸå¤±å€¼
    """
    dh = torch.abs(x[..., :, 1:] - x[..., :, :-1])    # æ°´å¹³æ¢¯åº¦
    dw = torch.abs(x[..., 1:, :] - x[..., :-1, :])    # å‚ç›´æ¢¯åº¦
    return alfa * torch.mean(2*dh[..., :-1, :] + dw[..., :, :-1])

# å­æ³¢åˆå§‹åŒ–å‡½æ•°
def wavelet_init(seismic_data, wavelet_length):
    """
    ä»åœ°éœ‡æ•°æ®ä¼°è®¡åˆå§‹å­æ³¢
    è¾“å…¥: seismic_data - åœ°éœ‡æ•°æ®
          wavelet_length - å­æ³¢é•¿åº¦
    è¾“å‡º: ä¼°è®¡çš„åˆå§‹å­æ³¢
    """
    # ç®€åŒ–å®ç°ï¼šä½¿ç”¨Rickerå­æ³¢ä½œä¸ºåˆå§‹ä¼°è®¡
    dt = 0.001
    t = np.arange(wavelet_length) * dt
    f0 = 30  # ä¸»é¢‘30Hz
    wav = (1 - 2*np.pi**2*f0**2*t**2) * np.exp(-np.pi**2*f0**2*t**2)
    return torch.tensor(wav, dtype=torch.float32).unsqueeze(0).unsqueeze(0)

print("âœ… è¾…åŠ©å‡½æ•°å®šä¹‰å®Œæˆ")

#############################################################################################################
### ç¬¬7éƒ¨åˆ†ï¼šç½‘ç»œåˆå§‹åŒ–
#############################################################################################################

print("\n" + "="*80)
print("ğŸ¤– ç¬¬7éƒ¨åˆ†ï¼šç½‘ç»œåˆå§‹åŒ–")
print("="*80)

# UNeté˜»æŠ—åæ¼”ç½‘ç»œ
print("ğŸ—ï¸  åˆå§‹åŒ–UNetåæ¼”ç½‘ç»œ...")
net = UNet(
    in_ch=2,                 # è¾“å…¥é€šé“ï¼š[æœ€å°äºŒä¹˜åˆå§‹è§£, è§‚æµ‹åœ°éœ‡æ•°æ®]
    out_ch=1,                # è¾“å‡ºé€šé“ï¼šé˜»æŠ—æ®‹å·®
    channels=[8, 16, 32, 64],
    skip_channels=[0, 8, 16, 32],
    use_sigmoid=True,        # è¾“å‡ºå½’ä¸€åŒ–åˆ°[0,1]
    use_norm=False
).to(device)

# Forwardå»ºæ¨¡ç½‘ç»œï¼ˆå­æ³¢å­¦ä¹ ï¼‰
print("âš¡ åˆå§‹åŒ–Forwardå»ºæ¨¡ç½‘ç»œ...")
forward_net = forward_model(nonlinearity="tanh").to(device)

print(f"âœ… ç½‘ç»œåˆå§‹åŒ–å®Œæˆ:")
print(f"   - UNetå‚æ•°é‡: {sum(p.numel() for p in net.parameters()):,}")
print(f"   - Forwardç½‘ç»œå‚æ•°é‡: {sum(p.numel() for p in forward_net.parameters()):,}")
print(f"   - è®¾å¤‡: {device}")

#############################################################################################################
### ç¬¬8éƒ¨åˆ†ï¼šè®­ç»ƒç®—æ³• - ä¸¤é˜¶æ®µè®­ç»ƒ
#############################################################################################################

if Train:
    print("\n" + "="*80)
    print("ğŸš€ ç¬¬8éƒ¨åˆ†ï¼šä¸¤é˜¶æ®µè®­ç»ƒç®—æ³•")
    print("="*80)
    
    # è®­ç»ƒå‚æ•°
    lr = 1e-3
    yita = 1e-1    # äº•çº¦æŸæŸå¤±æƒé‡
    mu = 5e-4      # TVæ­£åˆ™åŒ–æƒé‡
    beta = 0       # é¢å¤–ç›‘ç£æŸå¤±æƒé‡
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    optimizerF = torch.optim.Adam(forward_net.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9)
    
    # æŸå¤±å‡½æ•°
    mse = torch.nn.MSELoss()
    
    print(f"ğŸ“‹ è®­ç»ƒå‚æ•°é…ç½®:")
    print(f"   - å­¦ä¹ ç‡: {lr}")
    print(f"   - äº•çº¦æŸæƒé‡: {yita}")
    print(f"   - TVæ­£åˆ™åŒ–æƒé‡: {mu}")
    print(f"   - é˜¶æ®µ1è½®æ¬¡: {ADMM_ITER}")
    print(f"   - é˜¶æ®µ2è½®æ¬¡: {ADMM_ITER1}")
    
    #########################################################################################################
    ### é˜¶æ®µ1ï¼šå­æ³¢å­¦ä¹ 
    #########################################################################################################
    
    print("\n" + "-"*60)
    print("ğŸŒŠ é˜¶æ®µ1ï¼šæ•°æ®é©±åŠ¨çš„å­æ³¢å­¦ä¹ ")
    print("-"*60)
    print("ç›®æ ‡ï¼šåˆ©ç”¨å®Œæ•´é˜»æŠ—æ•°æ®ï¼ˆç»“åˆäº•ä½æ©ç ï¼‰å­¦ä¹ æœ€ä¼˜å­æ³¢")
    print("æŸå¤±ï¼šL_wavelet = ||M âŠ™ [ForwardNet(âˆ‡Z_full, w_0)]_synth - M âŠ™ S_obs||Â²")
    
    admm_iter = ADMM_ITER
    psnr_net_total = []
    total_lossF = []
    epsI = 0.1
    
    print("å¼€å§‹å­æ³¢å­¦ä¹ è®­ç»ƒ...")
    for i in range(admm_iter):
        epoch_loss = 0
        batch_count = 0
        
        # ä¿®æ­£ï¼šå˜é‡åå¯¹åº”æˆ‘ä»¬çš„è®¾è®¡
        for S_obs_batch, Z_full_batch, Z_back_batch, M_mask_batch in Train_loader:
            optimizerF.zero_grad()
            
            # é˜¶æ®µ1æ ¸å¿ƒï¼šåŠ æƒå­æ³¢å­¦ä¹ 
            # ç›®æ ‡ï¼šåœ¨é«˜å¯ä¿¡åº¦åŒºåŸŸï¼ˆM=1.0ï¼‰å­¦ä¹ æœ€ä¼˜å­æ³¢
            # å…¬å¼ï¼šL_wavelet = ||M âŠ™ [ForwardNet(âˆ‡Z_full, w_0)]_synth - M âŠ™ S_obs||Â²
            
            # è®¡ç®—å®Œæ•´é˜»æŠ—çš„åå°„ç³»æ•°
            reflection_coeff = DIFFZ(Z_full_batch)
            
            # Forwardç½‘ç»œï¼šè¾“å‡º[åˆæˆåœ°éœ‡æ•°æ®, å­¦ä¹ çš„å­æ³¢]
            synthetic_seismic, learned_wavelet = forward_net(
                reflection_coeff, 
                torch.tensor(wav0[None, None, :, None], device=device)
            )
            
            # åŠ æƒæŸå¤±ï¼šäº•ä½æ©ç ç¡®ä¿åœ¨é«˜å¯ä¿¡åº¦åŒºåŸŸä¸»å¯¼å­æ³¢å­¦ä¹ 
            # M_mask_batch=1.0çš„ä½ç½®ï¼šZ_full_batchä¸ºçœŸå®æµ‹äº•å€¼ï¼Œå¯ä¿¡åº¦é«˜
            # M_mask_batchâ‰ˆ0.0çš„ä½ç½®ï¼šZ_full_batchä¸ºæ’å€¼ä¼°è®¡å€¼ï¼Œå¯ä¿¡åº¦ä½
            lossF = mse(
                M_mask_batch * synthetic_seismic, 
                M_mask_batch * S_obs_batch
            ) * S_obs_batch.shape[3]
            
            lossF.backward()
            optimizerF.step()
            
            epoch_loss += lossF.item()
            batch_count += 1
        
        avg_loss = epoch_loss / batch_count
        total_lossF.append(avg_loss)
        
        if i % 20 == 0:
            print(f"   Epoch {i:04d}/{admm_iter:04d}, å­æ³¢å­¦ä¹ æŸå¤±: {avg_loss:.6f}")
            print(f"      è¯´æ˜ï¼šæŸå¤±è¶Šå°ï¼Œå­¦ä¹ çš„å­æ³¢åœ¨é«˜å¯ä¿¡åº¦åŒºåŸŸæ‹Ÿåˆè§‚æµ‹æ•°æ®è¶Šå¥½")
    
    # æå–å­¦ä¹ åˆ°çš„å­æ³¢
    print("ğŸ¯ æå–ä¼˜åŒ–åçš„å­æ³¢...")
    with torch.no_grad():
        _, wav_learned = forward_net(
            DIFFZ(Z_full_batch), 
            torch.tensor(wav0[None, None, :, None], device=device)
        )
        wav_learned_np = wav_learned.detach().cpu().squeeze().numpy()
    
    # å­æ³¢åå¤„ç†ï¼ˆé«˜æ–¯çª—å¹³æ»‘ï¼‰
    N = len(wav_learned_np)
    std = 25
    gaussian_window = gaussian(N, std)
    wav_learned_smooth = gaussian_window * (wav_learned_np - wav_learned_np.mean())
    
    print(f"âœ… é˜¶æ®µ1å®Œæˆï¼šå­æ³¢å­¦ä¹ ")
    print(f"   - è®­ç»ƒè½®æ¬¡: {admm_iter}")
    print(f"   - æœ€ç»ˆæŸå¤±: {total_lossF[-1]:.6f}")
    print(f"   - å­¦ä¹ å­æ³¢é•¿åº¦: {len(wav_learned_smooth)}")
    
    #########################################################################################################
    ### é˜¶æ®µ2ï¼šUNeté˜»æŠ—åæ¼”
    #########################################################################################################
    
    print("\n" + "-"*60)
    print("ğŸ¯ é˜¶æ®µ2ï¼šUNeté˜»æŠ—åæ¼”")
    print("-"*60)
    print("ç›®æ ‡ï¼šä½¿ç”¨å­¦ä¹ çš„å­æ³¢è¿›è¡Œé«˜ç²¾åº¦é˜»æŠ—åæ¼”")
    print("ç­–ç•¥ï¼šæœ€å°äºŒä¹˜åˆå§‹åŒ– + UNetæ®‹å·®å­¦ä¹ ")
    print("æŸå¤±ï¼šL_total = L_unsup + L_sup + L_tv")
    
    # æ„å»ºåŸºäºå­¦ä¹ å­æ³¢çš„å·ç§¯ç®—å­
    print("ğŸ”§ æ„å»ºå­¦ä¹ å­æ³¢çš„å·ç§¯ç®—å­...")
    nz = S_obs_batch.shape[2]
    S = torch.diag(0.5 * torch.ones(nz - 1), diagonal=1) - torch.diag(0.5 * torch.ones(nz - 1), diagonal=-1)
    S[0] = S[-1] = 0
    
    WW = pylops.utils.signalprocessing.convmtx(wav_learned_smooth/wav_learned_smooth.max(), size, len(wav_learned_smooth) // 2)[:size]
    WW = torch.tensor(WW, dtype=torch.float32, device=device)
    WW = WW @ S.to(device)
    PP = torch.matmul(WW.T, WW) + epsI * torch.eye(WW.shape[0], device=device)
    
    admm_iter1 = ADMM_ITER1
    print(f"å¼€å§‹UNetåæ¼”è®­ç»ƒ (å…±{admm_iter1}è½®)...")
    
    for i in range(admm_iter1):
        epoch_loss = 0
        epoch_loss_sup = 0
        epoch_loss_unsup = 0
        epoch_loss_tv = 0
        batch_count = 0
        
        # ä¿®æ­£ï¼šç¡®ä¿å˜é‡åå¯¹åº”è®¾è®¡
        for S_obs_batch, Z_full_batch, Z_back_batch, M_mask_batch in Train_loader:
            optimizer.zero_grad()
            
            # æ­¥éª¤1ï¼šæœ€å°äºŒä¹˜åˆå§‹åŒ–
            # ä½¿ç”¨å­¦ä¹ çš„å­æ³¢æ„å»ºçš„ç®—å­è¿›è¡Œåˆå§‹åŒ–
            # å…¬å¼ï¼šZ_init = argmin ||Wâˆ‡Z - (S_obs - Wâˆ‡Z_back)||Â² + Îµ||Z||Â²
            datarn = torch.matmul(WW.T, S_obs_batch - torch.matmul(WW, Z_back_batch))
            x, _, _, _ = torch.linalg.lstsq(PP[None, None], datarn)
            Z_init = x + Z_back_batch  # åŠ å›ä½é¢‘èƒŒæ™¯
            Z_init = (Z_init - Z_init.min()) / (Z_init.max() - Z_init.min())  # å½’ä¸€åŒ–
            
            # æ­¥éª¤2ï¼šUNetæ®‹å·®å­¦ä¹ 
            # UNetè¾“å…¥ï¼š[æœ€å°äºŒä¹˜åˆå§‹è§£, è§‚æµ‹åœ°éœ‡æ•°æ®]
            # UNetè¾“å‡ºï¼šé˜»æŠ—æ®‹å·® Î”Z
            # æœ€ç»ˆé¢„æµ‹ï¼šZ_pred = Î”Z + Z_init
            Z_pred = net(torch.cat([Z_init, S_obs_batch], dim=1)) + Z_init
            
            # ä¸‰é¡¹æŸå¤±å‡½æ•°è®¡ç®—
            
            # 1. äº•çº¦æŸæŸå¤±ï¼ˆå·®å¼‚åŒ–ç›‘ç£ï¼‰
            # å…¬å¼ï¼šL_sup = M âŠ™ ||Z_pred - Z_full||Â²
            # å«ä¹‰ï¼šåœ¨äº•ä½å¤„ï¼ˆM=1.0ï¼‰å¼ºåˆ¶åŒ¹é…çœŸå®æµ‹äº•å€¼
            #       åœ¨æ’å€¼å¤„ï¼ˆMâ‰ˆ0.0ï¼‰å‡ ä¹æ— ç›‘ç£çº¦æŸ
            loss_sup = yita * mse(
                M_mask_batch * Z_pred, 
                M_mask_batch * Z_full_batch
            ) * Z_full_batch.shape[3] / 3
            
            # 2. ç‰©ç†çº¦æŸæŸå¤±ï¼ˆæ­£æ¼”ä¸€è‡´æ€§ï¼‰
            # å…¬å¼ï¼šL_unsup = ||ForwardModel(Z_pred, W_learned) - S_obs||Â²
            # å«ä¹‰ï¼šç¡®ä¿é¢„æµ‹é˜»æŠ—çš„æ­£æ¼”ç»“æœä¸è§‚æµ‹åœ°éœ‡æ•°æ®ä¸€è‡´
            # è¯´æ˜ï¼šforward_netæ˜¯å­æ³¢æ ¡æ­£ç½‘ç»œï¼Œè¾“å…¥åˆå§‹å­æ³¢ï¼Œè¾“å‡ºæ ¡æ­£åçš„æœ€ä¼˜å­æ³¢
            pred_reflection = DIFFZ(Z_pred)
            pred_seismic, _ = forward_net(
                pred_reflection, 
                torch.tensor(wav0[None, None, :, None], device=device)  # å§‹ç»ˆä½¿ç”¨åˆå§‹å­æ³¢
            )
            loss_unsup = mse(pred_seismic, S_obs_batch)
            
            # 3. æ€»å˜åˆ†æ­£åˆ™åŒ–æŸå¤±ï¼ˆç©ºé—´å¹³æ»‘æ€§ï¼‰
            # å…¬å¼ï¼šL_tv = Î±Â·TV(Z_pred)
            # å«ä¹‰ï¼šä¿è¯åæ¼”ç»“æœçš„ç©ºé—´è¿ç»­æ€§
            loss_tv = tv_loss(Z_pred, mu)
            
            # æ€»æŸå¤±
            total_loss = loss_unsup + loss_tv + loss_sup
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)
            optimizer.step()
            scheduler.step()
            
            # è®°å½•æŸå¤±
            epoch_loss += total_loss.item()
            epoch_loss_sup += loss_sup.item()
            epoch_loss_unsup += loss_unsup.item()
            epoch_loss_tv += loss_tv.item()
            batch_count += 1
        
        # è¾“å‡ºè®­ç»ƒè¿›åº¦
        if i % 2 == 0:
            avg_total = epoch_loss / batch_count
            avg_sup = epoch_loss_sup / batch_count
            avg_unsup = epoch_loss_unsup / batch_count
            avg_tv = epoch_loss_tv / batch_count
            print(f"   Epoch {i:04d}/{admm_iter1:04d}")
            print(f"      æ€»æŸå¤±: {avg_total:.6f}")
            print(f"      äº•çº¦æŸæŸå¤±: {avg_sup:.6f} (é«˜å¯ä¿¡åº¦åŒºåŸŸåŒ¹é…)")
            print(f"      ç‰©ç†çº¦æŸæŸå¤±: {avg_unsup:.6f} (æ­£æ¼”ä¸€è‡´æ€§)")
            print(f"      TVæ­£åˆ™åŒ–æŸå¤±: {avg_tv:.6f} (ç©ºé—´å¹³æ»‘æ€§)")
    
    print(f"âœ… é˜¶æ®µ2å®Œæˆï¼šUNeté˜»æŠ—åæ¼”è®­ç»ƒ")
    
    # ä¿å­˜æ¨¡å‹
    save_path = 'logs/model/Uet_TV_IMP_7labels_channel3.pth'
    torch.save(net.state_dict(), save_path)
    print(f"ğŸ’¾ UNetæ¨¡å‹å·²ä¿å­˜: {save_path}")
    
    # ä¿å­˜Forwardç½‘ç»œï¼ˆå­æ³¢å­¦ä¹ ç½‘ç»œï¼‰
    forward_save_path = 'logs/model/forward_net_wavelet_learned.pth'
    torch.save(forward_net.state_dict(), forward_save_path)
    print(f"ğŸ’¾ Forwardç½‘ç»œå·²ä¿å­˜: {forward_save_path}")
    print(f"   è¯´æ˜ï¼šForwardç½‘ç»œåŒ…å«è®­ç»ƒæ—¶å­¦ä¹ çš„æœ€ä¼˜å­æ³¢å‚æ•°")

#############################################################################################################
### ç¬¬9éƒ¨åˆ†ï¼šæµ‹è¯•å’Œç»“æœè¯„ä¼°
#############################################################################################################

if not Train:
    print("\n" + "="*80)
    print("ğŸ” ç¬¬9éƒ¨åˆ†ï¼šæ¨¡å‹æµ‹è¯•å’Œç»“æœè¯„ä¼°")
    print("="*80)
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    save_path = 'logs/model/Uet_TV_IMP_7labels_channel3.pth'
    net.load_state_dict(torch.load(save_path, map_location=device))
    net.eval()
    print(f"âœ… UNetæ¨¡å‹åŠ è½½å®Œæˆ: {save_path}")
    
    # åŠ è½½é¢„è®­ç»ƒçš„Forwardç½‘ç»œï¼ˆå­æ³¢å­¦ä¹ ç½‘ç»œï¼‰
    forward_save_path = 'logs/model/forward_net_wavelet_learned.pth'
    try:
        forward_net.load_state_dict(torch.load(forward_save_path, map_location=device))
        forward_net.eval()
        print(f"âœ… Forwardç½‘ç»œåŠ è½½å®Œæˆ: {forward_save_path}")
        use_learned_wavelet = True
    except FileNotFoundError:
        print(f"âš ï¸  Forwardç½‘ç»œæ–‡ä»¶æœªæ‰¾åˆ°: {forward_save_path}")
        print("   ğŸ“ è§£å†³æ–¹æ¡ˆï¼š")
        print("      1. è®¾ç½® Train = True å¹¶è¿è¡Œè®­ç»ƒæ¥ç”ŸæˆForwardç½‘ç»œæƒé‡")
        print("      2. æˆ–è€…å½“å‰å°†ä½¿ç”¨åˆå§‹å­æ³¢è¿›è¡Œæ¨ç†ï¼ˆæ€§èƒ½å¯èƒ½ä¸‹é™ï¼‰")
        print("   ğŸ”§ å½“å‰é€‰æ‹©æ–¹æ¡ˆ2ï¼šä½¿ç”¨åˆå§‹å­æ³¢ç»§ç»­æ¨ç†")
        use_learned_wavelet = False
    
    # æ¨ç†é˜¶æ®µï¼šæ„å»ºå­æ³¢ç®—å­
    print("ğŸ”§ æ„å»ºæ¨ç†ç”¨å­æ³¢ç®—å­...")
    size = S_obs.shape[0]
    nz = size
    epsI = 0.1
    
    # æ„å»ºå·®åˆ†ç®—å­
    S = torch.diag(0.5 * torch.ones(nz - 1), diagonal=1) - torch.diag(0.5 * torch.ones(nz - 1), diagonal=-1)
    S[0] = S[-1] = 0
    
    if use_learned_wavelet:
        print("   ä½¿ç”¨è®­ç»ƒå¥½çš„Forwardç½‘ç»œè·å–å­¦ä¹ çš„å­æ³¢...")
        # ä½¿ç”¨è®­ç»ƒå¥½çš„forward_netè·å–å­¦ä¹ çš„å­æ³¢
        with torch.no_grad():
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæµ‹è¯•æ ·æœ¬è·å–å­¦ä¹ çš„å­æ³¢
            sample_S_obs = torch.tensor(S_obs_test_norm[..., None]).permute(2, 3, 0, 1).type(dtype)[:1]
            sample_Z_full = torch.tensor(Z_full_test_norm[..., None]).permute(2, 3, 0, 1).type(dtype)[:1]
            sample_reflection = DIFFZ(sample_Z_full)
            
            _, learned_wavelet = forward_net(
                sample_reflection, 
                torch.tensor(wav0[None, None, :, None], device=device)
            )
            wav_learned_np = learned_wavelet.detach().cpu().squeeze().numpy()
        
        # å­æ³¢åå¤„ç†ï¼ˆé«˜æ–¯çª—å¹³æ»‘ï¼‰
        N = len(wav_learned_np)
        std = 25
        gaussian_window = gaussian(N, std)
        wav_final = gaussian_window * (wav_learned_np - wav_learned_np.mean())
        wav_final = wav_final / wav_final.max()  # å½’ä¸€åŒ–
        
        print(f"   âœ… ä½¿ç”¨å­¦ä¹ çš„å­æ³¢: é•¿åº¦={len(wav_final)}")
    else:
        print("   ä½¿ç”¨åˆå§‹å­æ³¢...")
        wav_final = wav0 / wav0.max()
        print(f"   âš ï¸  ä½¿ç”¨åˆå§‹å­æ³¢: é•¿åº¦={len(wav_final)}")
    
    # ä½¿ç”¨æœ€ç»ˆå­æ³¢æ„å»ºå·ç§¯ç®—å­
    WW = pylops.utils.signalprocessing.convmtx(wav_final, size, len(wav_final) // 2)[:size]
    WW = torch.tensor(WW, dtype=torch.float32, device=device)
    WW = WW @ S.to(device)
    PP = torch.matmul(WW.T, WW) + epsI * torch.eye(WW.shape[0], device=device)
    
    print(f"âœ… æ¨ç†å­æ³¢ç®—å­æ„å»ºå®Œæˆ:")
    print(f"   - å­æ³¢é•¿åº¦: {len(wav_final)}")
    print(f"   - å·ç§¯ç®—å­å½¢çŠ¶: {WW.shape}")
    print(f"   - å­æ³¢ç±»å‹: {'å­¦ä¹ çš„å­æ³¢' if use_learned_wavelet else 'åˆå§‹å­æ³¢'}")
    
    print("ğŸ” å¼€å§‹æµ‹è¯•...")
    # æ–°å¢ï¼šæ”¶é›†æ‰€æœ‰æ‰¹æ¬¡ï¼Œæ‹¼æˆ3Dä½“
    all_pred = []
    all_true = []
    all_input = []
    all_back = []  # æ”¶é›†ä½é¢‘èƒŒæ™¯é˜»æŠ—
    all_sesimic=[]  ##æ”¶é›†è§‚æµ‹åœ°éœ‡æ•°æ®
    with torch.no_grad():
        for batch_idx, (test_S_obs, test_Z_full, test_Z_back, test_Z_true) in enumerate(Test_loader):
            datarn = torch.matmul(WW.T, test_S_obs - torch.matmul(WW, test_Z_back))
            x, _, _, _ = torch.linalg.lstsq(PP[None, None], datarn)
            Z_init = x + test_Z_back
            Z_init = (Z_init - Z_init.min()) / (Z_init.max() - Z_init.min())
            Z_pred = net(torch.cat([Z_init, test_S_obs], dim=1)) + Z_init
            all_pred.append(Z_pred.cpu().numpy())
            all_true.append(test_Z_full.cpu().numpy())
            all_input.append(test_S_obs.cpu().numpy())
            all_back.append(test_Z_back.cpu().numpy())  # æ–°å¢
            all_sesimic.append(test_S_obs.cpu().numpy())
            print(f"   å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(Test_loader)}")
    print("âœ… æµ‹è¯•å®Œæˆ")
    
    # æ‹¼æˆ3Dä½“ [N, 1, time, space] -> [N, time, space]
    all_pred = np.concatenate(all_pred, axis=0)
    all_true = np.concatenate(all_true, axis=0)
    all_input = np.concatenate(all_input, axis=0)
    all_back = np.concatenate(all_back, axis=0)  # æ–°å¢
    all_pred = np.squeeze(all_pred, axis=1)
    all_true = np.squeeze(all_true, axis=1)
    all_input = np.squeeze(all_input, axis=1)
    all_back = np.squeeze(all_back, axis=1)  # æ–°å¢
    all_sesimic = np.squeeze(all_sesimic, axis=1)  # æ–°å¢
    # åå½’ä¸€åŒ–
    all_pred_imp = np.exp(all_pred * (logimpmax - logimpmin) + logimpmin)
    all_true_imp = np.exp(all_true * (logimpmax - logimpmin) + logimpmin)
    all_back_imp = np.exp(all_back * (logimpmax - logimpmin) + logimpmin)
    ##å¯¹åœ°éœ‡æ•°æ®å•ç‹¬å½’ä¸€åŒ–
    all_sesimic = (all_sesimic - all_sesimic.min()) / (all_sesimic.max() - all_sesimic.min())
    # ä¿å­˜ä¸º3Dä½“
    print(f"\nğŸ’¾ ä¿å­˜æ¨ç†ç»“æœ3Dæ•°æ®...")
    np.save('logs/results/prediction_sample.npy', all_pred)
    np.save('logs/results/true_sample.npy', all_true)
    np.save('logs/results/input_sample.npy', all_input)
    np.save('logs/results/seismic_record.npy', all_sesimic)
    
    np.save('logs/results/prediction_impedance.npy', all_pred_imp)
    np.save('logs/results/true_impedance.npy', all_true_imp)
    np.save('logs/results/background_impedance.npy', all_back_imp)  # æ–°å¢
    print(f"   âœ… æ¨ç†3Dæ•°æ®å·²ä¿å­˜: logs/results/prediction_impedance.npy, logs/results/true_impedance.npy ç­‰ shape: {all_pred_imp.shape}")
    print(f"   âœ… ä½é¢‘èƒŒæ™¯é˜»æŠ—å·²ä¿å­˜: logs/results/background_impedance.npy shape: {all_back.shape}")
    print("\n" + "="*80)
    print("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ")
    print("="*80)

