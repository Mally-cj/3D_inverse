"""
ç®€åŒ–ç‰ˆåœ°éœ‡é˜»æŠ—åæ¼”ä¸»ç¨‹åº
ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®å¤„ç†æ¨¡å—å’Œç¼“å­˜æœºåˆ¶
"""

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import sys
import torch.optim
from Model.net2D import UNet, forward_model
from Model.utils import DIFFZ, tv_loss, wavelet_init
import matplotlib.pyplot as plt
import numpy as np
import pylops
from pylops.utils.wavelets import ricker
from scipy.signal import filtfilt
from scipy import signal
import torch.nn.functional as F
from scipy.signal.windows import gaussian
import pdb
import sys
sys.path.append('..')
from icecream import ic 
sys.path.append('../codes')
sys.path.append('deep_learning_impedance_inversion_chl')
import psutil
import gc
from tqdm import tqdm

# å¯¼å…¥æ•°æ®å¤„ç†æ¨¡å—
from data_processor import SeismicDataProcessor

# è®­ç»ƒ/æµ‹è¯•æ¨¡å¼åˆ‡æ¢
Train = False  # è®¾ç½®ä¸º True è¿›è¡Œè®­ç»ƒå¹¶ä¿å­˜Forwardç½‘ç»œæƒé‡ï¼›è®¾ç½®ä¸º False è¿›è¡Œæ¨ç†æµ‹è¯•

# ğŸ“ é‡è¦è¯´æ˜ï¼š
# - é¦–æ¬¡ä½¿ç”¨æ—¶ï¼Œå»ºè®®å…ˆè®¾ç½® Train = True è¿è¡Œä¸€æ¬¡è®­ç»ƒï¼Œç”ŸæˆForwardç½‘ç»œæƒé‡æ–‡ä»¶
# - Forwardç½‘ç»œå­¦ä¹ æ•°æ®é©±åŠ¨çš„æœ€ä¼˜å­æ³¢ï¼Œå¯¹åæ¼”ç²¾åº¦è‡³å…³é‡è¦
# - è®­ç»ƒå®Œæˆåï¼Œå¯è®¾ç½® Train = False è¿›è¡Œæ¨ç†æµ‹è¯•

# æ™ºèƒ½è®¾å¤‡æ£€æµ‹å’Œå‚æ•°é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ Using device: {device}")

# æ ¹æ®è®¾å¤‡è‡ªåŠ¨è°ƒæ•´å‚æ•°
if device.type == 'cuda':
    print("ğŸ“Š GPU mode: Using full dataset")
    dtype = torch.cuda.FloatTensor
    # GPUå‚æ•°é…ç½®
    ADMM_ITER = 100
    ADMM_ITER1 = 50
else:
    print("ğŸ’» CPU mode: Using optimized subset")
    dtype = torch.FloatTensor
    # CPUä¼˜åŒ–å‚æ•°é…ç½®
    ADMM_ITER = 30
    ADMM_ITER1 = 15

print(f"ğŸ“‹ Configuration:")
print(f"  - Training iterations: {ADMM_ITER} + {ADMM_ITER1}")

#############################################################################################################
### ç¬¬1éƒ¨åˆ†ï¼šæ•°æ®å¤„ç† - ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®å¤„ç†æ¨¡å—
#############################################################################################################

print("\n" + "="*80) 
print("ğŸ“‚ ç¬¬1éƒ¨åˆ†ï¼šæ•°æ®å¤„ç†")
print("="*80)

# ä¸€é”®å¤„ç†æ‰€æœ‰æ•°æ®
from data_processor import SeismicDataProcessor

processor = SeismicDataProcessor(cache_dir='cache', device='auto')

data_info = None  # å…ˆåˆå§‹åŒ–
if Train:
    train_loader, norm_params, data_info = processor.process_train_data()
    print(f"   - è®­ç»ƒæ‰¹æ•°: {len(train_loader)}")
else:
    # ç”¨patch loaderå’Œç´¢å¼•
    test_loader, indices, shape3d, norm_params = processor.process_test_data()
    # æ‰‹åŠ¨è¡¥å……data_infoï¼ˆç”¨äºåç»­shapeç­‰ä¿¡æ¯ï¼‰
    impedance_model_full = processor.load_impedance_data()
    S_obs = processor.load_seismic_data()
    well_pos, _, _ = processor.generate_well_mask(S_obs)
    data_info = {
        'impedance_shape': impedance_model_full.shape,
        'seismic_shape': S_obs.shape,
        'well_positions': well_pos
    }
    print(f"   - æµ‹è¯•æ‰¹æ•°: {len(test_loader)}")

# æå–å½’ä¸€åŒ–å‚æ•°
logimpmax = norm_params['logimpmax']
logimpmin = norm_params['logimpmin']

print(f"ğŸ“Š æ•°æ®å¤„ç†å®Œæˆ:")
print(f"   - é˜»æŠ—æ•°æ®å½¢çŠ¶: {data_info['impedance_shape']}")
print(f"   - åœ°éœ‡æ•°æ®å½¢çŠ¶: {data_info['seismic_shape']}")
print(f"   - äº•ä½æ•°é‡: {len(data_info['well_positions'])}")



#############################################################################################################
### ç¬¬3éƒ¨åˆ†ï¼šç½‘ç»œåˆå§‹åŒ–
#############################################################################################################

print("\n" + "="*80)
print("ğŸ¤– ç¬¬3éƒ¨åˆ†ï¼šç½‘ç»œåˆå§‹åŒ–")
print("="*80)

# UNeté˜»æŠ—åæ¼”ç½‘ç»œ
print("ğŸ—ï¸  åˆå§‹åŒ–UNetåæ¼”ç½‘ç»œ...")
net = UNet(
    in_ch=2,                 # è¾“å…¥é€šé“ï¼š[æœ€å°äºŒä¹˜åˆå§‹è§£, è§‚æµ‹åœ°éœ‡æ•°æ®]
    out_ch=1,                # è¾“å‡ºé€šé“ï¼šé˜»æŠ—æ®‹å·®
    channels=[8, 16, 32, 64],
    skip_channels=[0, 8, 16, 32],
    use_sigmoid=True,        # è¾“å‡ºå½’ä¸€åŒ–åˆ°[0,1]
    use_norm=False
).to(device)

# Forwardå»ºæ¨¡ç½‘ç»œï¼ˆå­æ³¢å­¦ä¹ ï¼‰
print("âš¡ åˆå§‹åŒ–Forwardå»ºæ¨¡ç½‘ç»œ...")
forward_net = forward_model(nonlinearity="tanh").to(device)

print(f"âœ… ç½‘ç»œåˆå§‹åŒ–å®Œæˆ:")
print(f"   - UNetå‚æ•°é‡: {sum(p.numel() for p in net.parameters()):,}")
print(f"   - Forwardç½‘ç»œå‚æ•°é‡: {sum(p.numel() for p in forward_net.parameters()):,}")
print(f"   - è®¾å¤‡: {device}")

#############################################################################################################
### ç¬¬4éƒ¨åˆ†ï¼šè®­ç»ƒç®—æ³• - ä¸¤é˜¶æ®µè®­ç»ƒ
#############################################################################################################

if Train:
    print("\n" + "="*80)
    print("ğŸš€ ç¬¬4éƒ¨åˆ†ï¼šä¸¤é˜¶æ®µè®­ç»ƒç®—æ³•")
    print("="*80)
    
    # è®­ç»ƒå‚æ•°
    lr = 1e-3
    yita = 1e-1    # äº•çº¦æŸæŸå¤±æƒé‡
    mu = 5e-4      # TVæ­£åˆ™åŒ–æƒé‡
    beta = 0       # é¢å¤–ç›‘ç£æŸå¤±æƒé‡
    size = data_info['seismic_shape'][0]
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    optimizerF = torch.optim.Adam(forward_net.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9)
    
    # æŸå¤±å‡½æ•°
    mse = torch.nn.MSELoss()
    
    print(f"ğŸ“‹ è®­ç»ƒå‚æ•°é…ç½®:")
    print(f"   - å­¦ä¹ ç‡: {lr}")
    print(f"   - äº•çº¦æŸæƒé‡: {yita}")
    print(f"   - TVæ­£åˆ™åŒ–æƒé‡: {mu}")
    print(f"   - é˜¶æ®µ1è½®æ¬¡: {ADMM_ITER}")
    print(f"   - é˜¶æ®µ2è½®æ¬¡: {ADMM_ITER1}")
    
    #########################################################################################################
    ### é˜¶æ®µ1ï¼šå­æ³¢çŸ«æ­£å™¨ï¼ˆForwardNetï¼‰å­¦ä¹ æœ€ä¼˜å­æ³¢
    #########################################################################################################
    
    print("\n" + "-"*60)
    print("ğŸŒŠ é˜¶æ®µ1ï¼šå­æ³¢çŸ«æ­£å™¨(ForwardNet)å­¦ä¹ æœ€ä¼˜å­æ³¢")
    print("-"*60)
    print("ç›®æ ‡ï¼šåˆ©ç”¨äº•ä½é«˜å¯ä¿¡åº¦åŒºåŸŸï¼Œé€šè¿‡ForwardNet(å­æ³¢çŸ«æ­£å™¨)è‡ªé€‚åº”è°ƒæ•´åˆå§‹å­æ³¢ï¼Œä½¿å…¶æ›´è´´åˆå®é™…åœ°éœ‡å“åº”")
    print("æŸå¤±ï¼šL_wavelet = ||M âŠ™ [ForwardNet(âˆ‡Z_full, w_0)]_synth - M âŠ™ S_obs||Â²")
    
    admm_iter = ADMM_ITER
    psnr_net_total = []
    total_lossF = []
    epsI = 0.1
    wav0 = wavelet_init(101).squeeze().numpy()
    print("å¼€å§‹å­æ³¢çŸ«æ­£å™¨è®­ç»ƒ...")
    for i in range(admm_iter):
        epoch_loss = 0
        batch_count = 0
        
        for S_obs_batch, Z_full_batch, Z_back_batch, M_mask_batch in train_loader:
            optimizerF.zero_grad()
            # è®¡ç®—å®Œæ•´é˜»æŠ—çš„åå°„ç³»æ•°
            reflection_coeff = DIFFZ(Z_full_batch)
            # ForwardNet(å­æ³¢çŸ«æ­£å™¨)ï¼šè¾“å…¥åå°„ç³»æ•°å’Œåˆå§‹å­æ³¢ï¼Œè¾“å‡ºçŸ«æ­£åå­æ³¢å¹¶åˆæˆåœ°éœ‡
            synthetic_seismic, learned_wavelet = forward_net(
                reflection_coeff, 
                torch.tensor(wav0[None, None, :, None], device=device)
            )
            # æŸå¤±ï¼šåˆæˆåœ°éœ‡ä¸è§‚æµ‹åœ°éœ‡çš„æ©ç åŠ æƒMSE
            lossF = mse(
                M_mask_batch * synthetic_seismic, 
                M_mask_batch * S_obs_batch
            ) * S_obs_batch.shape[3]
            lossF.backward()
            optimizerF.step()
            epoch_loss += lossF.item()
            batch_count += 1
        avg_loss = epoch_loss / batch_count
        total_lossF.append(avg_loss)
        if i % 20 == 0:
            print(f"   Epoch {i:04d}/{admm_iter:04d}, å­æ³¢çŸ«æ­£æŸå¤±: {avg_loss:.6f}")
            print(f"      è¯´æ˜ï¼šæŸå¤±è¶Šå°ï¼ŒForwardNetè¾“å‡ºçš„çŸ«æ­£å­æ³¢åœ¨é«˜å¯ä¿¡åº¦åŒºåŸŸæ‹Ÿåˆè§‚æµ‹æ•°æ®è¶Šå¥½")
    # æå–çŸ«æ­£åçš„å­æ³¢
    print("ğŸ¯ æå–ForwardNetçŸ«æ­£åçš„å­æ³¢...")
    with torch.no_grad():
        _, wav_learned = forward_net(
            DIFFZ(Z_full_batch), 
            torch.tensor(wav0[None, None, :, None], device=device)
        )
        wav_learned_np = wav_learned.detach().cpu().squeeze().numpy()
    # å­æ³¢åå¤„ç†ï¼ˆé«˜æ–¯çª—å¹³æ»‘ï¼‰
    N = len(wav_learned_np) # çª—çš„é•¿åº¦
    std = 25  # æ ‡å‡†å·®ï¼Œå†³å®šçª—çš„å®½åº¦
    gaussian_window = gaussian(N, std)
    #å¯¹å­æ³¢è¿›è¡Œä¸€ä¸ªçª—å£å¹³æ»‘ï¼Œå¯ä»¥ä½¿å­æ³¢ä¼°è®¡ç»“æœæ›´ç¨³å¥ï¼Œå› ä¸ºä¸Šè¿°çš„å­æ³¢æ¨¡å—ä¼šå‡ºç°è¾¹ç•Œä¸å¹³æ»‘ï¼Œå¯èƒ½è¿˜å¾—ä»”ç»†è°ƒè°ƒç½‘ç»œå’ŒæŸå¤±å‡½æ•°
    wav_learned_smooth = gaussian_window * (wav_learned_np - wav_learned_np.mean())
    print(f"âœ… é˜¶æ®µ1å®Œæˆï¼šForwardNetçŸ«æ­£å­æ³¢å­¦ä¹ ")
    print(f"   - è®­ç»ƒè½®æ¬¡: {admm_iter}")
    print(f"   - æœ€ç»ˆæŸå¤±: {total_lossF[-1]:.6f}")
    print(f"   - çŸ«æ­£åå­æ³¢é•¿åº¦: {len(wav_learned_smooth)}")
    #########################################################################################################
    ### é˜¶æ®µ2ï¼šUNeté˜»æŠ—åæ¼”
    #########################################################################################################
    print("\n" + "-"*60)
    print("ğŸ¯ é˜¶æ®µ2ï¼šUNeté˜»æŠ—åæ¼”")
    print("-"*60)
    print("ç›®æ ‡ï¼šä½¿ç”¨ForwardNetçŸ«æ­£åçš„å­æ³¢è¿›è¡Œé«˜ç²¾åº¦é˜»æŠ—åæ¼”")
    print("ç­–ç•¥ï¼šæœ€å°äºŒä¹˜åˆå§‹åŒ– + UNetæ®‹å·®å­¦ä¹ ")
    print("æŸå¤±ï¼šL_total = L_unsup + L_sup + L_tv")
    # æ„å»ºåŸºäºçŸ«æ­£å­æ³¢çš„å·ç§¯ç®—å­
    print("ğŸ”§ æ„å»ºForwardNetçŸ«æ­£åçš„å­æ³¢çš„å·ç§¯ç®—å­...")
    nz = S_obs_batch.shape[2]
    S = torch.diag(0.5 * torch.ones(nz - 1), diagonal=1) - torch.diag(0.5 * torch.ones(nz - 1), diagonal=-1)
    S[0] = S[-1] = 0
    WW = pylops.utils.signalprocessing.convmtx(wav_learned_smooth/wav_learned_smooth.max(), size, len(wav_learned_smooth) // 2)[:size]
    WW = torch.tensor(WW, dtype=torch.float32, device=device)
    WW = WW @ S.to(device)
    PP = torch.matmul(WW.T, WW) + epsI * torch.eye(WW.shape[0], device=device) ##æœ€å°äºŒä¹˜è§£çš„ToplitzçŸ©é˜µçš„è£…ç½®
    admm_iter1 = ADMM_ITER1
    print(f"å¼€å§‹UNetåæ¼”è®­ç»ƒ (å…±{admm_iter1}è½®)...")
    for i in range(admm_iter1):
        epoch_loss = 0
        epoch_loss_sup = 0
        epoch_loss_unsup = 0
        epoch_loss_tv = 0
        batch_count = 0
        for S_obs_batch, Z_full_batch, Z_back_batch, M_mask_batch in train_loader:
            optimizer.zero_grad()
            # æ­¥éª¤1ï¼šæœ€å°äºŒä¹˜åˆå§‹åŒ–
            datarn = torch.matmul(WW.T, S_obs_batch - torch.matmul(WW, Z_back_batch))
            x, _, _, _ = torch.linalg.lstsq(PP[None, None], datarn)
            Z_init = x + Z_back_batch  # åŠ å›ä½é¢‘èƒŒæ™¯
            Z_init = (Z_init - Z_init.min()) / (Z_init.max() - Z_init.min())  # å½’ä¸€åŒ–
            # æ­¥éª¤2ï¼šUNetæ®‹å·®å­¦ä¹ 
            Z_pred = net(torch.cat([Z_init, S_obs_batch], dim=1)) + Z_init
            # ä¸‰é¡¹æŸå¤±å‡½æ•°è®¡ç®—
            # 1. äº•çº¦æŸæŸå¤±ï¼ˆå·®å¼‚åŒ–ç›‘ç£ï¼‰
            loss_sup = yita * mse(
                M_mask_batch * Z_pred, 
                M_mask_batch * Z_full_batch
            ) * Z_full_batch.shape[3] / 3
            # 2. ç‰©ç†çº¦æŸæŸå¤±ï¼ˆæ­£æ¼”ä¸€è‡´æ€§ï¼‰
            pred_reflection = DIFFZ(Z_pred)
            pred_seismic, _ = forward_net(
                pred_reflection, 
                torch.tensor(wav0[None, None, :, None], device=device)  # å§‹ç»ˆç”¨åˆå§‹å­æ³¢åšæ­£æ¼”ä¸€è‡´æ€§
            )
            loss_unsup = mse(pred_seismic, S_obs_batch)
            # 3. æ€»å˜åˆ†æ­£åˆ™åŒ–æŸå¤±ï¼ˆç©ºé—´å¹³æ»‘æ€§ï¼‰
            loss_tv = tv_loss(Z_pred, mu)
            # æ€»æŸå¤±
            total_loss = loss_unsup + loss_tv + loss_sup
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)
            optimizer.step()
            scheduler.step()
            epoch_loss += total_loss.item()
            epoch_loss_sup += loss_sup.item()
            epoch_loss_unsup += loss_unsup.item()
            epoch_loss_tv += loss_tv.item()
            batch_count += 1
        if i % 2 == 0:
            avg_total = epoch_loss / batch_count
            avg_sup = epoch_loss_sup / batch_count
            avg_unsup = epoch_loss_unsup / batch_count
            avg_tv = epoch_loss_tv / batch_count
            print(f"   Epoch {i:04d}/{admm_iter1:04d}")
            print(f"      æ€»æŸå¤±: {avg_total:.6f}")
            print(f"      äº•çº¦æŸæŸå¤±: {avg_sup:.6f} (é«˜å¯ä¿¡åº¦åŒºåŸŸåŒ¹é…)")
            print(f"      ç‰©ç†çº¦æŸæŸå¤±: {avg_unsup:.6f} (æ­£æ¼”ä¸€è‡´æ€§)")
            print(f"      TVæ­£åˆ™åŒ–æŸå¤±: {avg_tv:.6f} (ç©ºé—´å¹³æ»‘æ€§)")
    print(f"âœ… é˜¶æ®µ2å®Œæˆï¼šUNeté˜»æŠ—åæ¼”è®­ç»ƒ")
    # ä¿å­˜æ¨¡å‹
    save_path = 'logs/model/Uet_TV_IMP_7labels_channel3.pth'
    torch.save(net.state_dict(), save_path)
    print(f"ğŸ’¾ UNetæ¨¡å‹å·²ä¿å­˜: {save_path}")
    # ä¿å­˜Forwardç½‘ç»œï¼ˆå­æ³¢çŸ«æ­£å™¨ï¼‰
    forward_save_path = 'logs/model/forward_net_wavelet_learned.pth'
    torch.save(forward_net.state_dict(), forward_save_path)
    print(f"ğŸ’¾ ForwardNet(å­æ³¢çŸ«æ­£å™¨)å·²ä¿å­˜: {forward_save_path}")
    print(f"   è¯´æ˜ï¼šForwardNetåŒ…å«è®­ç»ƒæ—¶å­¦ä¹ çš„çŸ«æ­£å­æ³¢å‚æ•°")

#############################################################################################################
### ç¬¬5éƒ¨åˆ†ï¼šæµ‹è¯•å’Œç»“æœè¯„ä¼°
#############################################################################################################
elif not Train:
    print("\n" + "="*80)
    print("ğŸ” ç¬¬5éƒ¨åˆ†ï¼šæ¨¡å‹æµ‹è¯•å’Œç»“æœè¯„ä¼°")
    print("="*80)
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    save_path = 'logs/model/Uet_TV_IMP_7labels_channel3.pth'
    net.load_state_dict(torch.load(save_path, map_location=device))
    net.eval()
    print(f"âœ… UNetæ¨¡å‹åŠ è½½å®Œæˆ: {save_path}")
    
    # åŠ è½½é¢„è®­ç»ƒçš„Forwardç½‘ç»œï¼ˆå­æ³¢å­¦ä¹ ç½‘ç»œï¼‰
    forward_save_path = 'logs/model/forward_net_wavelet_learned.pth'
    try:
        forward_net.load_state_dict(torch.load(forward_save_path, map_location=device))
        forward_net.eval()
        print(f"âœ… Forwardç½‘ç»œåŠ è½½å®Œæˆ: {forward_save_path}")
        use_learned_wavelet = True
    except FileNotFoundError:
        print(f"âš ï¸  Forwardç½‘ç»œæ–‡ä»¶æœªæ‰¾åˆ°: {forward_save_path}")
    
    # æ¨ç†é˜¶æ®µï¼šæ„å»ºå­æ³¢ç®—å­
    print("ğŸ”§ æ„å»ºæ¨ç†ç”¨å­æ³¢ç®—å­...")
    wav0 = wavelet_init(101).squeeze().numpy()
    wav00=torch.tensor(wav0[None, None, :, None],device=device)
    size = data_info['seismic_shape'][0]
    nz = size
    epsI = 0.1
    S = torch.diag(0.5 * torch.ones(nz - 1), diagonal=1) - torch.diag(0.5 * torch.ones(nz - 1), diagonal=-1)
    S=S.to(device)
    S[0] = S[-1] = 0

    
    wav_learned_np= forward_net(wav00, wav00)[1].detach().cpu().squeeze().numpy()
    N = len(wav_learned_np)
    std = 25
    gaussian_window = gaussian(N, std)
    wav_final = gaussian_window * (wav_learned_np - wav_learned_np.mean())
    wav_final = wav_final / wav_final.max()
    WW = pylops.utils.signalprocessing.convmtx(wav_final, size, len(wav_final) // 2)[:size]
    WW = torch.tensor(WW,device=device)
    WW = WW.float()
    S = S.float()
    WW = WW @ S
    PP = torch.matmul(WW.T, WW) + epsI * torch.eye(WW.shape[0], device=device)
    print(f"âœ… æ¨ç†å­æ³¢ç®—å­æ„å»ºå®Œæˆ:")
    print(f"   - å­æ³¢é•¿åº¦: {len(wav_final)}")
    print(f"   - å·ç§¯ç®—å­å½¢çŠ¶: {WW.shape}")
    print(f"   - å­æ³¢ç±»å‹: {'å­¦ä¹ çš„å­æ³¢' if use_learned_wavelet else 'åˆå§‹å­æ³¢'}")
    print("ğŸ” å¼€å§‹æµ‹è¯•patchæ¨ç†...")
    # 1. è·å–patch loaderã€ç´¢å¼•ã€shapeã€å½’ä¸€åŒ–å‚æ•°
    test_loader, indices, shape3d, norm_params = processor.process_test_data()
    
    # 2. æ¨ç†å¾ªç¯
    pred_patch_list = []
    true_patch_list = []
    input_patch_list = []
    back_patch_list = []
    seismic_patch_list = []
    logimpmax = norm_params['logimpmax']
    logimpmin = norm_params['logimpmin']
    with torch.no_grad():
        for i, (s_patch, imp_patch, zback_patch) in enumerate(test_loader):
            s_patch = s_patch.to(device)
            imp_patch = imp_patch.to(device)
            zback_patch = zback_patch.to(device)
            # æœ€å°äºŒä¹˜åˆå§‹åŒ–
            datarn = torch.matmul(WW.T, s_patch - torch.matmul(WW, zback_patch))
            x, _, _, _ = torch.linalg.lstsq(PP[None, None], datarn)
            Z_init = x + zback_patch
            Z_init = (Z_init - Z_init.min()) / (Z_init.max() - Z_init.min())
            # ç½‘ç»œæ¨ç†
            Z_pred = net(torch.cat([Z_init, s_patch], dim=1)) + Z_init
            # æ”¶é›†patchç»“æœï¼ˆé€‚é…batch>1ï¼‰
            Z_pred_np = Z_pred.cpu().numpy()  # [batch, 1, time, patch_size]
            imp_patch_np = imp_patch.cpu().numpy()
            zback_patch_np = zback_patch.cpu().numpy()
            s_patch_np = s_patch.cpu().numpy()
            # squeezeæ‰é€šé“ç»´ï¼ˆaxis=1ï¼‰ï¼Œéå†batch
            Z_pred_np = np.squeeze(Z_pred_np, axis=1)  # [batch, time, patch_size]
            imp_patch_np = np.squeeze(imp_patch_np, axis=1)
            zback_patch_np = np.squeeze(zback_patch_np, axis=1)
            s_patch_np = np.squeeze(s_patch_np, axis=1)
            for b in range(Z_pred_np.shape[0]):
                pred_patch_list.append(Z_pred_np[b])
                true_patch_list.append(imp_patch_np[b])
                back_patch_list.append(zback_patch_np[b])
                seismic_patch_list.append(s_patch_np[b])
            current_patches = len(pred_patch_list)
            print(f"   å¤„ç†patch {current_patches}/{len(indices)}")
    
    # 3. æ‹¼å›3Dä½“
    print(f"   æ‹¼å›3Dä½“...") # [N, time, patch_size]
    pred_3d = processor.reconstruct_3d_from_patches(pred_patch_list, indices)
    true_3d = processor.reconstruct_3d_from_patches(true_patch_list, indices)
    back_3d = processor.reconstruct_3d_from_patches(back_patch_list, indices)
    seismic_3d = processor.reconstruct_3d_from_patches(seismic_patch_list, indices)
    
    # 4. åå½’ä¸€åŒ–
    # pred_3d_imp = np.exp(pred_3d * (logimpmax - logimpmin) + logimpmin)
    # true_3d_imp = np.exp(true_3d * (logimpmax - logimpmin) + logimpmin)
    # back_3d_imp = np.exp(back_3d * (logimpmax - logimpmin) + logimpmin)
    pred_3d_imp = pred_3d
    true_3d_imp = true_3d
    back_3d_imp = back_3d
    
    # 5. ä¿å­˜æ‰€æœ‰å¯è§†åŒ–éœ€è¦çš„æ–‡ä»¶
    os.makedirs('logs/results', exist_ok=True)
    # np.save('logs/results/prediction_sample.npy', np.array(pred_patch_list))
    # np.save('logs/results/true_sample.npy', np.array(true_patch_list))
    # np.save('logs/results/input_sample.npy', np.array(seismic_patch_list))
    np.save('logs/results/seismic_record.npy', seismic_3d)
    np.save('logs/results/prediction_impedance.npy', pred_3d_imp)
    np.save('logs/results/true_impedance.npy', true_3d_imp)
    np.save('logs/results/background_impedance.npy', back_3d_imp)
    print(f"   âœ… æ¨ç†æ•°æ®å·²ä¿å­˜: logs/results/prediction_impedance.npy ç­‰ shape: {pred_3d_imp.shape}")
    print("\n" + "="*80)
    print("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ")
    print("="*80) 